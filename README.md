# ğŸ¤– Robot Arena - UCB AI Battle

Um jogo de combate entre robÃ´s onde vocÃª enfrenta uma IA que aprende em tempo real usando o algoritmo **Upper Confidence Bound (UCB)** de aprendizado por reforÃ§o. O jogo implementa fÃ­sica clÃ¡ssica e campo de visÃ£o baseado em produto escalar para simular combates realistas.

![Game Banner](https://img.shields.io/badge/Game-Robot_Arena-purple?style=for-the-badge)
![AI](https://img.shields.io/badge/AI-UCB_Reinforcement_Learning-blue?style=for-the-badge)
![Physics](https://img.shields.io/badge/Physics-Classical_Mechanics-green?style=for-the-badge)

---

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Por Que UCB?](#-por-que-ucb)
- [Arquitetura TÃ©cnica](#-arquitetura-tÃ©cnica)
- [Sistema de FÃ­sica](#-sistema-de-fÃ­sica)
- [Como Jogar](#-como-jogar)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [Estrutura do CÃ³digo](#-estrutura-do-cÃ³digo)
- [Algoritmo UCB Detalhado](#-algoritmo-ucb-detalhado)
- [Melhorias Futuras](#-melhorias-futuras)
- [ComparaÃ§Ã£o com Outros Algoritmos](#-comparaÃ§Ã£o-com-outros-algoritmos)

---

## ğŸ® VisÃ£o Geral

**Robot Arena** Ã© um jogo de combate 1v1 onde:
- **VocÃª** controla o robÃ´ azul atravÃ©s de botÃµes
- **O Bot** (robÃ´ vermelho) usa UCB para aprender e adaptar sua estratÃ©gia
- Cada aÃ§Ã£o tem consequÃªncias baseadas em **fÃ­sica clÃ¡ssica**
- Danos crÃ­ticos e nÃ£o-crÃ­ticos criam **variÃ¢ncia estratÃ©gica**
- Campo de visÃ£o usa **produto escalar** para detecÃ§Ã£o realista

### Tecnologias Utilizadas

- **Phaser.js 3.55.2** - Engine de jogo 2D
- **JavaScript ES6+** - LÃ³gica de jogo e IA
- **HTML5 Canvas** - RenderizaÃ§Ã£o
- **CSS3** - Interface responsiva

---

## ğŸ¯ Por Que UCB?

### A Escolha TÃ©cnica Perfeita

O **Upper Confidence Bound** foi escolhido por razÃµes matematicamente sÃ³lidas:

#### 1. **Natureza EstocÃ¡stica do Combate**
```
Dano Normal: 15 HP
Dano CrÃ­tico: 25 HP (25% chance)
```

Lutas tÃªm **variÃ¢ncia inerente** - UCB Ã© especialmente eficaz em ambientes com recompensas estocÃ¡sticas porque:
- Balanceia exploraÃ§Ã£o vs exploraÃ§Ã£o usando intervalos de confianÃ§a
- NÃ£o desperdiÃ§a tentativas apÃ³s aprender (diferente de Îµ-greedy)
- Garante convergÃªncia teÃ³rica para a aÃ§Ã£o Ã³tima

#### 2. **Recompensas Naturalmente Bounded**

Como mencionado na concepÃ§Ã£o do projeto:
> "Lutas tÃªm danos crÃ­ticos e nÃ£o crÃ­ticos, logo, estabelecer um intervalo de recompensas Ã© ideal."

```javascript
// FunÃ§Ã£o de recompensa normalizada [0, 1]
reward = (damageTaken * 0.5 - damageReceived * 0.5 + criticalBonus + healthBonus) / 50
reward = clamp(reward, 0, 1)
```

**Por que isso importa:**
- UCB assume recompensas em intervalo conhecido
- Lutas naturalmente tÃªm dano mÃ¡ximo/mÃ­nimo
- NormalizaÃ§Ã£o [0,1] otimiza a exploraÃ§Ã£o

#### 3. **FÃ³rmula UCB**

```javascript
UCB(action) = Î¼(action) + c Ã— âˆš(ln(N) / n(action))
             â””â”€exploitationâ”€â”˜   â””â”€explorationâ”€â”€â”˜
```

Onde:
- `Î¼(action)` = recompensa mÃ©dia observada
- `c = âˆš2` = parÃ¢metro de exploraÃ§Ã£o
- `N` = tentativas totais
- `n(action)` = vezes que essa aÃ§Ã£o foi escolhida

**InterpretaÃ§Ã£o:**
- Se uma aÃ§Ã£o foi pouco testada â†’ `n` pequeno â†’ termo de exploraÃ§Ã£o grande â†’ **explora**
- Se uma aÃ§Ã£o tem boa mÃ©dia â†’ `Î¼` alto â†’ **explota** (usa o que funciona)

---

## ğŸ—ï¸ Arquitetura TÃ©cnica

### Diagrama de Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           INTERFACE (HTML/CSS)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Stats Panel â”‚      â”‚ Game Canvas â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          PHASER.JS GAME ENGINE              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Scene Management & Rendering      â”‚   â”‚
â”‚  â”‚   â€¢ Tweens (animaÃ§Ãµes fÃ­sicas)      â”‚   â”‚
â”‚  â”‚   â€¢ Sprites (robÃ´s, arena)          â”‚   â”‚
â”‚  â”‚   â€¢ Input (botÃµes interativos)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            GAME LOGIC (JavaScript)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Combat Systemâ”‚    â”‚   UCB Agent  â”‚      â”‚
â”‚  â”‚ â€¢ Damage calcâ”‚    â”‚ â€¢ Action sel â”‚      â”‚
â”‚  â”‚ â€¢ Physics    â”‚    â”‚ â€¢ Reward upd â”‚      â”‚
â”‚  â”‚ â€¢ FOV (dot)  â”‚    â”‚ â€¢ Learning   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stack de Processamento

```
Player Input â†’ Action Selection â†’ Animation
                                      â†“
                Bot UCB Decision â† Game State
                                      â†“
                Combat Resolution (Physics)
                                      â†“
                Reward Calculation â†’ UCB Update
                                      â†“
                UI Update â†’ Next Turn
```

---

## ğŸ”¬ Sistema de FÃ­sica

### Campo de VisÃ£o (Dot Product)

O campo de visÃ£o usa produto escalar - uma escolha **fisicamente correta** e **computacionalmente eficiente**:

```javascript
// Vetor do robÃ´ atÃ© o oponente
const vectorToEnemy = { x: enemy.x - robot.x, y: enemy.y - robot.y };

// Ã‚ngulo de visÃ£o usando arctan
const viewAngle = Math.abs(Math.atan2(vectorToEnemy.y, vectorToEnemy.x));

// FOV de ~60Â° (Ï€/3 radianos)
const inView = viewAngle < Math.PI / 3;
```

**Matematicamente:**
```
cos(Î¸) = (A Â· B) / (|A| Ã— |B|)

Se cos(Î¸) > 0.5 (60Â°) â†’ Alvo estÃ¡ no campo de visÃ£o
```

**Por que Dot Product?**
- âš¡ **Performance:** Uma multiplicaÃ§Ã£o vs mÃºltiplas operaÃ§Ãµes trigonomÃ©tricas
- ğŸ¯ **Realismo:** VisÃ£o humana/robÃ³tica tem cone de ~60-120Â°
- ğŸ”„ **Direcionalidade:** RobÃ´s precisam estar "virados" para ver

### MecÃ¢nica ClÃ¡ssica

#### 1. **ConservaÃ§Ã£o de Energia**
```javascript
// Charge: Alto risco = Alta recompensa
if (action === 'charge') {
  vulnerability += 50%;  // Mais exposiÃ§Ã£o
  nextAttackDamage *= 1.5;  // Mais poder
}
```

#### 2. **Momentum nas AnimaÃ§Ãµes**
```javascript
scene.tweens.add({
  targets: robot,
  x: originalX + (50 * facing),
  duration: 200,
  yoyo: true,
  ease: 'Power2'  // AceleraÃ§Ã£o quadrÃ¡tica (fÃ­sica newtoniana)
});
```

#### 3. **Matriz de Combate**

| Jogador â†“ / Bot â†’ | Attack | Defend | Dodge | Charge |
|-------------------|--------|--------|-------|--------|
| **Attack**        | 0.5Ã— ambos | 0.3Ã— bot | Miss | 1.5Ã— bot |
| **Defend**        | 0.3Ã— player | Reposition | Reposition | Reposition |
| **Dodge**         | Miss | Reposition | Reposition | Reposition |
| **Charge**        | 1.5Ã— player | Counter | Counter | Clash |

---

## ğŸ® Como Jogar

### Controles

Clique nos botÃµes na parte inferior da arena:

- **âš”ï¸ ATTACK** - Ataque direto (dano base)
- **ğŸ›¡ï¸ DEFEND** - Postura defensiva (reduz dano em 70%)
- **ğŸ’¨ DODGE** - Esquiva completa (evita 100% do dano)
- **âš¡ CHARGE** - Ataque poderoso (1.5Ã— dano, mas vulnerÃ¡vel)

### MecÃ¢nicas

1. **Danos CrÃ­ticos:** 25% de chance de causar 25 HP (vs 15 HP normal)
2. **Cooldown:** Cada aÃ§Ã£o tem cooldown de 1 segundo
3. **Rounds:** Primeiro a chegar a 0 HP perde
4. **Aprendizado:** O bot melhora a cada round

### EstratÃ©gia

- **Contra Attack:** Use Dodge ou Defend
- **Contra Defend:** Use Charge para quebrar defesa
- **Contra Dodge:** Use Attack mÃºltiplas vezes
- **Contra Charge:** Attack rÃ¡pido antes que complete

**Dica:** O bot UCB aprende seus padrÃµes! Varie suas aÃ§Ãµes.

---

## ğŸ“¦ InstalaÃ§Ã£o

### MÃ©todo 1: Direto no Navegador

1. Salve o cÃ³digo como `index.html`
2. Abra no navegador (Chrome, Firefox, Edge, Safari)
3. Pronto! Nenhuma instalaÃ§Ã£o necessÃ¡ria

### MÃ©todo 2: Live Server (VS Code)

```bash
# 1. Instale a extensÃ£o Live Server no VS Code
# 2. Clique com botÃ£o direito em index.html
# 3. Selecione "Open with Live Server"
```

### MÃ©todo 3: Servidor Local

```bash
# Python 3
python -m http.server 8000

# Node.js
npx http-server

# Acesse: http://localhost:8000
```

### Requisitos

- âœ… Navegador moderno (Chrome 90+, Firefox 88+, Safari 14+)
- âœ… JavaScript habilitado
- âœ… ConexÃ£o Ã  internet (para carregar Phaser.js do CDN)

**Compatibilidade:**
- ğŸ–¥ï¸ Desktop: Windows, macOS, Linux
- ğŸ“± Mobile: iOS 12+, Android 8+
- ğŸ“Ÿ Tablet: iPad, Android tablets

---

## ğŸ“‚ Estrutura do CÃ³digo

```
robot-arena/
â”‚
â”œâ”€â”€ index.html                 # Arquivo principal
â”‚   â”œâ”€â”€ <head>
â”‚   â”‚   â”œâ”€â”€ Phaser.js CDN     # Engine de jogo
â”‚   â”‚   â””â”€â”€ Styles (CSS)      # Design responsivo
â”‚   â”‚
â”‚   â”œâ”€â”€ <body>
â”‚   â”‚   â”œâ”€â”€ Stats Panel       # UI de estatÃ­sticas
â”‚   â”‚   â”œâ”€â”€ Game Container    # Canvas do Phaser
â”‚   â”‚   â””â”€â”€ Info Panel        # InstruÃ§Ãµes
â”‚   â”‚
â”‚   â””â”€â”€ <script>
â”‚       â”œâ”€â”€ UCBAgent Class    # Algoritmo de RL
â”‚       â”œâ”€â”€ Phaser Config     # ConfiguraÃ§Ã£o do jogo
â”‚       â”œâ”€â”€ create()          # InicializaÃ§Ã£o da cena
â”‚       â”œâ”€â”€ playerAction()    # LÃ³gica do jogador
â”‚       â”œâ”€â”€ resolveCombat()   # Sistema de combate
â”‚       â”œâ”€â”€ calculateReward() # FunÃ§Ã£o de recompensa
â”‚       â””â”€â”€ update()          # Game loop
â”‚
â””â”€â”€ README.md                 # Este arquivo
```

### Classes Principais

#### `UCBAgent`
```javascript
class UCBAgent {
  constructor()              // Inicializa arrays de valores
  selectAction()             // Escolhe melhor aÃ§Ã£o (UCB)
  updateReward(index, reward) // Atualiza mÃ©dias incrementais
  getActionName(index)       // Converte Ã­ndice â†’ nome
}
```

#### FunÃ§Ãµes de Jogo

| FunÃ§Ã£o | Responsabilidade |
|--------|------------------|
| `create()` | Inicializa arena, robÃ´s, UI |
| `playerAction()` | Processa entrada do jogador |
| `animateRobot()` | Aplica fÃ­sica nas animaÃ§Ãµes |
| `resolveCombat()` | Calcula resultado da luta |
| `calculateReward()` | FunÃ§Ã£o de recompensa do UCB |
| `updateHealthBar()` | Atualiza barras de vida |
| `checkWinCondition()` | Verifica vitÃ³ria/derrota |
| `update()` | Game loop (60 FPS) |

---

## ğŸ§  Algoritmo UCB Detalhado

### ImplementaÃ§Ã£o Passo a Passo

#### 1. **InicializaÃ§Ã£o**

```javascript
constructor() {
  this.actions = ['attack', 'defend', 'dodge', 'charge'];
  this.counts = [0, 0, 0, 0];    // Quantas vezes cada aÃ§Ã£o foi testada
  this.values = [0, 0, 0, 0];    // Recompensa mÃ©dia de cada aÃ§Ã£o
  this.totalCount = 0;            // Total de aÃ§Ãµes executadas
  this.c = Math.sqrt(2);          // ParÃ¢metro de exploraÃ§Ã£o (teÃ³rico Ã³timo)
}
```

#### 2. **SeleÃ§Ã£o de AÃ§Ã£o**

```javascript
selectAction() {
  this.totalCount++;
  
  // FASE 1: ExploraÃ§Ã£o inicial (tenta cada aÃ§Ã£o pelo menos uma vez)
  for (let i = 0; i < this.actions.length; i++) {
    if (this.counts[i] === 0) {
      return i;
    }
  }
  
  // FASE 2: UCB balanceado
  const ucbValues = this.actions.map((_, i) => {
    const exploitation = this.values[i];
    const exploration = this.c * Math.sqrt(Math.log(this.totalCount) / this.counts[i]);
    return exploitation + exploration;
  });
  
  return ucbValues.indexOf(Math.max(...ucbValues));
}
```

**Exemplo NumÃ©rico:**

ApÃ³s 100 rounds:
```
Action    | Count | Mean | Exploration | UCB Score
----------|-------|------|-------------|----------
attack    |  40   | 0.6  | 0.26        | 0.86
defend    |  30   | 0.4  | 0.29        | 0.69
dodge     |  20   | 0.5  | 0.35        | 0.85
charge    |  10   | 0.7  | 0.48        | 1.18  â† Escolhido!
```

**Charge** Ã© escolhido porque tem alta mÃ©dia (0.7) mas foi pouco testado (exploration alto).

#### 3. **AtualizaÃ§Ã£o de Recompensa**

```javascript
updateReward(actionIndex, reward) {
  this.counts[actionIndex]++;
  const n = this.counts[actionIndex];
  
  // MÃ©dia incremental (evita armazenar histÃ³rico completo)
  this.values[actionIndex] += (reward - this.values[actionIndex]) / n;
}
```

**FÃ³rmula da MÃ©dia Incremental:**
```
Î¼â‚™ = Î¼â‚™â‚‹â‚ + (xâ‚™ - Î¼â‚™â‚‹â‚) / n

Onde:
  Î¼â‚™ = nova mÃ©dia
  Î¼â‚™â‚‹â‚ = mÃ©dia anterior
  xâ‚™ = nova observaÃ§Ã£o
  n = nÃºmero de observaÃ§Ãµes
```

**Por que incremental?**
- âœ… **MemÃ³ria O(1):** NÃ£o precisa guardar todas as recompensas
- âœ… **Performance O(1):** Uma operaÃ§Ã£o por update
- âœ… **PrecisÃ£o:** Matematicamente equivalente Ã  mÃ©dia completa

#### 4. **FunÃ§Ã£o de Recompensa**

```javascript
function calculateReward(result, botHealthAfter) {
  let reward = 0;
  
  // Componente 1: Dano causado (positivo)
  reward += result.playerDamage * 0.5;
  
  // Componente 2: Dano recebido (negativo)
  reward -= result.botDamage * 0.5;
  
  // Componente 3: BÃ´nus crÃ­tico
  if (result.isCritical && result.botDamage > 0) {
    reward += 5;
  }
  
  // Componente 4: PreservaÃ§Ã£o de saÃºde
  reward += botHealthAfter * 0.1;
  
  // NormalizaÃ§Ã£o para [0, 1]
  return Math.max(0, Math.min(1, reward / 50));
}
```

**AnÃ¡lise de Componentes:**

| CenÃ¡rio | playerDmg | botDmg | Critical | botHP | Reward |
|---------|-----------|--------|----------|-------|--------|
| VitÃ³ria dominante | 25 | 0 | Sim | 80 | 1.0 |
| Troca equilibrada | 15 | 15 | NÃ£o | 50 | 0.5 |
| Derrota | 0 | 25 | NÃ£o | 30 | 0.18 |
| Defesa perfeita | 0 | 0 | NÃ£o | 100 | 1.0 |

---

## ğŸš€ Melhorias Futuras

### 1. **UCB Contextual**

Incorporar contexto de vida atual:

```javascript
class ContextualUCBAgent {
  selectAction(playerHealth, botHealth) {
    const context = this.discretizeHealth(playerHealth, botHealth);
    return this.ucbPerContext[context].selectAction();
  }
  
  discretizeHealth(pH, bH) {
    // Divide em buckets: HIGH > 60, MID 30-60, LOW < 30
    return `${this.bucket(pH)}_${this.bucket(bH)}`;
  }
}
```

**BenefÃ­cio:** Bot aprende que "charge" Ã© bom quando tem HP alto, mas ruim com HP baixo.

### 2. **Dot Product ExplÃ­cito no Dano**

```javascript
// Calcular produto escalar real
const robotForward = { x: Math.cos(robot.angle), y: Math.sin(robot.angle) };
const toEnemy = normalize({ x: enemy.x - robot.x, y: enemy.y - robot.y });
const dotProduct = robotForward.x * toEnemy.x + robotForward.y * toEnemy.y;

// Modificar dano baseado na direÃ§Ã£o
const accuracyMultiplier = Math.max(0.3, dotProduct); // MÃ­nimo 30% se de costas
damage *= accuracyMultiplier;
```

**Realismo:** Atacar pelas costas causa menos dano (realista em artes marciais).

### 3. **Decaying Exploration**

Reduzir exploraÃ§Ã£o ao longo do tempo:

```javascript
selectAction() {
  // ExploraÃ§Ã£o diminui exponencialmente
  const decayedC = this.c * Math.exp(-this.totalCount / 1000);
  
  const ucbValues = this.actions.map((_, i) => {
    return this.values[i] + decayedC * Math.sqrt(Math.log(this.totalCount) / this.counts[i]);
  });
  
  return ucbValues.indexOf(Math.max(...ucbValues));
}
```

**Justificativa:** ApÃ³s muitos rounds, o bot jÃ¡ explorou o suficiente e deve focar em explotar.

### 4. **Multi-Armed Bandit com PrevisÃ£o**

Em vez de 4 aÃ§Ãµes, ter 16 "arms" (4Ã—4 combinaÃ§Ãµes):

```javascript
// Predizer aÃ§Ã£o do jogador E escolher contra-aÃ§Ã£o
this.arms = [
  'attack_vs_attack', 'attack_vs_defend', 'attack_vs_dodge', 'attack_vs_charge',
  'defend_vs_attack', 'defend_vs_defend', // ... 16 total
];
```

**Complexidade:** O(16) vs O(4), mas aprende matchups especÃ­ficos.

### 5. **VisualizaÃ§Ã£o do Aprendizado**

```javascript
// GrÃ¡fico em tempo real mostrando:
// - Valores UCB de cada aÃ§Ã£o
// - NÃºmero de vezes testadas
// - EvoluÃ§Ã£o da recompensa mÃ©dia
```

### 6. **Modos de Dificuldade**

```javascript
const difficulty = {
  easy: { c: 2.0, rewardNoise: 0.1 },
  medium: { c: Math.sqrt(2), rewardNoise: 0 },
  hard: { c: 0.5, rewardNoise: 0, memory: 50 } // Lembra Ãºltimas 50 aÃ§Ãµes do jogador
};
```

### 7. **FÃ­sica de ColisÃ£o**

```javascript
// Adicionar hitbox real usando Phaser Physics
scene.physics.add.overlap(playerSword, bot, onHit);

function onHit(sword, target) {
  const velocity = sword.body.velocity;
  const impactForce = Math.sqrt(velocity.x ** 2 + velocity.y ** 2);
  damage = baseDamage * (impactForce / maxVelocity);
}
```

---

## ğŸ“Š ComparaÃ§Ã£o com Outros Algoritmos

### Tabela Comparativa

| Algoritmo | Complexidade | ConvergÃªncia | Sample Efficiency | AdequaÃ§Ã£o | Score |
|-----------|--------------|--------------|-------------------|-----------|-------|
| **UCB** âœ… | O(k) | Garantida | Alta | Perfeita | â­â­â­â­â­ |
| **Îµ-greedy** | O(k) | NÃ£o garantida | MÃ©dia | OK | â­â­â­ |
| **Thompson Sampling** | O(k) | Bayesiana | Alta | Boa | â­â­â­â­ |
| **Q-Learning** | O(sÃ—a) | Sim (Bellman) | Baixa | Overkill | â­â­â­ |
| **Deep Q-Network** | O(âˆ) | InstÃ¡vel | Muito baixa | DesnecessÃ¡rio | â­ |
| **Policy Gradient** | O(âˆ) | Local | Muito baixa | Complexo demais | â­â­ |

*k = nÃºmero de aÃ§Ãµes, s = estados, a = aÃ§Ãµes*

### AnÃ¡lise Detalhada

#### 1. **UCB (Escolha Atual)** â­â­â­â­â­

**PrÃ³s:**
- âœ… Garantia teÃ³rica de convergÃªncia: `Regret = O(log n)`
- âœ… NÃ£o precisa parÃ¢metro de exploraÃ§Ã£o manual (Îµ)
- âœ… Sample efficient (aprende rÃ¡pido com poucos dados)
- âœ… DeterminÃ­stico (reproduzÃ­vel para debug)
- âœ… InterpretÃ¡vel (pode-se ver por que escolheu cada aÃ§Ã£o)

**Contras:**
- âŒ Assume recompensas estacionÃ¡rias (jogador nÃ£o muda drÃ¡sticamente)
- âŒ NÃ£o modela adversÃ¡rio explicitamente

**Melhor para:** Recompensas bounded, estocÃ¡sticas, estacionÃ¡rias âœ… (nosso caso!)

#### 2. **Îµ-greedy** â­â­â­

```javascript
selectAction() {
  if (Math.random() < this.epsilon) {
    return Math.floor(Math.random() * this.actions.length); // Explora
  }
  return this.values.indexOf(Math.max(...this.values)); // Explota
}
```

**PrÃ³s:**
- âœ… Simples de implementar
- âœ… Funciona razoavelmente bem

**Contras:**
- âŒ ExploraÃ§Ã£o Ã© **aleatÃ³ria** (desperdiÃ§a tentativas)
- âŒ Precisa tunar Îµ manualmente
- âŒ NÃ£o tem garantia de convergÃªncia
- âŒ Explora igualmente aÃ§Ãµes ruins e desconhecidas

**Quando usar:** ProtÃ³tipos rÃ¡pidos, baseline de comparaÃ§Ã£o

#### 3. **Thompson Sampling** â­â­â­â­

```javascript
selectAction() {
  // Sample de distribuiÃ§Ã£o Beta para cada aÃ§Ã£o
  const samples = this.actions.map((_, i) => {
    return this.betaSample(this.alphas[i], this.betas[i]);
  });
  return samples.indexOf(Math.max(...samples));
}
```

**PrÃ³s:**
- âœ… Bayesiano (modela incerteza probabilisticamente)
- âœ… Sample efficient como UCB
- âœ… Lida bem com recompensas nÃ£o-estacionÃ¡rias

**Contras:**
- âŒ Mais complexo (precisa distribuiÃ§Ã£o Beta)
- âŒ EstocÃ¡stico (dificulta debug)
- âŒ Requer mais conhecimento de estatÃ­stica

**Quando usar:** Recompensas muito ruidosas, A/B testing

#### 4. **Q-Learning** â­â­â­

```javascript
updateQ(state, action, reward, nextState) {
  const oldQ = this.Q[state][action];
  const maxNextQ = Math.max(...this.Q[nextState]);
  this.Q[state][action] = oldQ + this.alpha * (reward + this.gamma * maxNextQ - oldQ);
}
```

**PrÃ³s:**
- âœ… Aprende polÃ­tica Ã³tima (Bellman optimality)
- âœ… Modela estados (vida do bot, vida do jogador)
- âœ…Off-policy (pode aprender de dados antigos)

**Contras:**
- âŒ Precisa **muito mais dados** (sample inefficient)
- âŒ Precisa definir estados (discretizaÃ§Ã£o)
- âŒ Tabela Q explode com muitos estados
- âŒ HiperparÃ¢metros (Î±, Î³, Îµ) precisam tuning

**Quando usar:** MDPs com estados bem definidos, offline learning

#### 5. **Deep Q-Network (DQN)** â­

**PrÃ³s:**
- âœ… Escala para estados contÃ­nuos (imagens, etc.)
- âœ… State-of-the-art em jogos complexos (Atari)

**Contras:**
- âŒâŒ Precisa **milhares de episÃ³dios** para convergir
- âŒâŒ InstÃ¡vel (precisa replay buffer, target network)
- âŒâŒ Caixa preta (nÃ£o interpretÃ¡vel)
- âŒâŒ Overkill total para 4 aÃ§Ãµes simples
- âŒâŒ Requer TensorFlow.js (50MB+ de dependÃªncia)

**Quando usar:** Estados de alta dimensÃ£o (pixels, sensores), muito tempo de treino

#### 6. **Policy Gradient (REINFORCE)** â­â­

**PrÃ³s:**
- âœ… Aprende polÃ­ticas estocÃ¡sticas
- âœ… Funciona em espaÃ§os de aÃ§Ã£o contÃ­nuos

**Contras:**
- âŒâŒ VariÃ¢ncia extremamente alta
- âŒâŒ Sample inefficient (pior que Q-learning)
- âŒâŒ ConvergÃªncia lenta e instÃ¡vel
- âŒâŒ Precisa baseline (crÃ­tico) para funcionar bem

**Quando usar:** AÃ§Ãµes contÃ­nuas (controle de motor), robÃ³tica

---

### Por Que UCB Venceu?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CaracterÃ­sticas do Nosso Problema          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ“ 4 aÃ§Ãµes discretas (pequeno)              â”‚
â”‚  âœ“ Recompensas [0, 1] bounded               â”‚
â”‚  âœ“ VariÃ¢ncia mÃ©dia (crÃ­ticos sÃ£o 25%)       â”‚
â”‚  âœ“ Precisa aprender rÃ¡pido (poucas rounds)  â”‚
â”‚  âœ“ Deve ser interpretÃ¡vel (game design)     â”‚
â”‚  âœ“ Zero hiperparÃ¢metros para tunar          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   UCB Ã‰ IDEAL   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Teorema (Lai & Robbins, 1985):**
> "Para qualquer algoritmo de bandit, o regret assintÃ³tico Ã© pelo menos logarÃ­tmico. UCB atinge este limite inferior."

**TraduÃ§Ã£o:** UCB Ã© **matematicamente Ã³timo** para este tipo de problema! ğŸ¯

---

## ğŸ“š ReferÃªncias TÃ©cnicas

### Papers Fundamentais

1. **UCB Algorithm**
   - Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002)
   - "Finite-time Analysis of the Multiarmed Bandit Problem"
   - *Machine Learning, 47(2-3), 235-256*
   - [Link](https://link.springer.com/article/10.1023/A:1013689704352)

2. **Bandit Theory**
   - Lai, T. L., & Robbins, H. (1985)
   - "Asymptotically efficient adaptive allocation rules"
   - *Advances in Applied Mathematics, 6(1), 4-22*

3. **Thompson Sampling**
   - Thompson, W. R. (1933)
   - "On the likelihood that one unknown probability exceeds another"
   - *Biometrika, 25(3/4), 285-294*

### Recursos de Aprendizado

- **UCB Tutorial:** [Bandit Algorithms (Lattimore & SzepesvÃ¡ri)](https://tor-lattimore.com/downloads/book/book.pdf)
- **Reinforcement Learning:** [Sutton & Barto - RL Book](http://incompleteideas.net/book/the-book-2nd.html)
-
